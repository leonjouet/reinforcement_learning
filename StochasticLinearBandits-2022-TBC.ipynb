{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njfnT-sUER9J"
   },
   "source": [
    "# Lab on Stochastic Linear Bandits :\n",
    "\n",
    "We provide the environment to run a standard linear bandit experiment. The objective of this lab session is to understand how to implement LinUCB, the algorithm seen in class and its variant LinTS. We shall see that in practice there are some shortcomings in the implementation to make it efficient so we will guide you to obtain a working version. \n",
    "\n",
    "Questions are inline in the notebook and some reserved space are allocated for answers, but feel free to add cells for remarks and run your own experiments to test hypotheses you may have. \n",
    "\n",
    "\n",
    "**Report instructions**: Please export this notebook as a .tex file. Then, edit it and properly compile it into a pdf and \n",
    "only submit what is below the section ``Linear Bandit Agents''. Please make sure your name and that of your co-author are either indicated in the section head below or in the tex title commands.\n",
    "Finally, please check your document before submitting. Given the volume of reports, we cannot guarantee to follow up on corrupted or unreadable reports. \n",
    "\n",
    "Note: To export, you might need to follow the steps at https://nbconvert.readthedocs.io/en/latest/install.html#installing-tex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylZIq1mOJBr8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from model import Environment, Agent\n",
    "from display import plot_regret\n",
    "\n",
    "\n",
    "from scipy.stats import bernoulli\n",
    "from math import log\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HH892IKv95t3"
   },
   "source": [
    "# Action generators \n",
    "The provided function helps the environment to generate random action sets of size K in dimension d. All actions are renormalized. You can see an example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKTTMwwB8rQS"
   },
   "outputs": [],
   "source": [
    "def randomActionsGenerator(K,d, mean=None):\n",
    "    \"\"\"\n",
    "    K: int -- number of action vectors to be generated\n",
    "    d : int -- dimension of the action space\n",
    "    returns : an array of K vectors uniformly sampled on the unit sphere in R^d\n",
    "    \"\"\"\n",
    "    if mean is None:\n",
    "        mean = np.zeros(d)\n",
    "    vecs = np.random.multivariate_normal(mean, np.eye(d), size=K)\n",
    "    norms = np.linalg.norm(vecs,axis=1)\n",
    "    return vecs / norms[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRMn0Zrx9rvX"
   },
   "outputs": [],
   "source": [
    "a = randomActionsGenerator(20,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "UeLSv2Vp9-75",
    "outputId": "0c6e20e2-009c-4d15-afdf-10dd2b306a6b"
   },
   "outputs": [],
   "source": [
    "plt.scatter(a[:,0],a[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3phj8yu79_Rt"
   },
   "source": [
    "# Environment Class\n",
    "\n",
    "The environment class allows to create 3 types of linear bandit problems: \n",
    "* 'fixed' : normally requires a fixed_actions input (otherwise randomly generated at start) which is kept all along the game;\n",
    "* 'arbitrary': at each round, an 'arbitrary' set of actions is chosen and here we decided to simply create a pool of (d x K) vectors and let the environment choose K of them without replacement at each round;\n",
    "* 'iid' : at each round, the environment samples K actions at random on the sphere.\n",
    "\n",
    "For each of these types of game, the class is used to generate the action sets at each round and the reward for a chosen action (chosen by an Agent, see the \"Play!\" section for the details of the interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKlD-rMUKumJ"
   },
   "outputs": [],
   "source": [
    "class LinearBandit(Environment):\n",
    "    \n",
    "    def __init__(self, theta, K, pb_type, model='gaussian', var=1., fixed_actions=None):\n",
    "        \"\"\"\n",
    "        theta: d-dimensional vector (bounded) representing the hidden parameter\n",
    "        K: number of actions per round (random action vectors generated each time)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.d = np.size(theta)\n",
    "        self.theta = theta\n",
    "        self.K = K\n",
    "        self.var = var\n",
    "        self.current_action_set = np.zeros(self.d)\n",
    "        self.pb_type = pb_type\n",
    "        if self.pb_type not in ['fixed', 'iid', 'arbitrary']:\n",
    "            raise ValueError(pb_type, \"this type of problem is unknown, either change it or define it :)\")\n",
    "            \n",
    "        #Now, set up the game for the given type:\n",
    "        if self.pb_type == 'fixed':\n",
    "            if fixed_actions is not None:\n",
    "\n",
    "                self.K, action_dim = fixed_actions.shape #safety reset of self.K in case different\n",
    "                if action_dim != self.d: # safety check\n",
    "                    raise ValueError(fixed_actions, \"actions dimension and theta mismatch: fix your actions.\")\n",
    "                self.fixed_actions = fixed_actions\n",
    "            else:\n",
    "                #create a fixed action set:\n",
    "                self.fixed_actions = randomActionsGenerator(self.K, self.d)\n",
    "            \n",
    "            self.current_action_set = self.fixed_actions\n",
    "            \n",
    "        elif self.pb_type == 'arbitrary':\n",
    "            #generate d*K actions and then sample K from this set at each round\n",
    "            # Note any other way to generate an arbitrary sequence can be used\n",
    "            self.pool_of_actions = randomActionsGenerator(self.d * self.K, self.d)\n",
    "            # select K actions without replacement in the pool:\n",
    "            self.current_action_set = self.pool_of_actions[np.random.choice(self.d*self.K, self.K, replace=False)]\n",
    "        elif self.pb_type == 'iid':\n",
    "            # generate a random set\n",
    "            self.current_action_set = randomActionsGenerator(self.K, self.d)       \n",
    "            \n",
    "        \n",
    "    def get_action_set(self):\n",
    "        if self.pb_type == 'fixed':   \n",
    "            self.current_action_set = self.fixed_actions\n",
    "            return self.current_action_set\n",
    "        elif self.pb_type == 'iid':\n",
    "            self.current_action_set = randomActionsGenerator(self.K, self.d)\n",
    "            return self.current_action_set\n",
    "        elif self.pb_type == 'arbitrary':\n",
    "            self.current_action_set = self.pool_of_actions[np.random.choice(self.d*self.K, self.K, replace=False)]\n",
    "            return self.current_action_set\n",
    "        \n",
    "        \n",
    "    def get_reward(self, action):\n",
    "    \n",
    "        \"\"\" sample reward given action and the model of this bandit environment\n",
    "        action: d-dimensional vector (action chosen by the learner)\n",
    "        \"\"\"\n",
    "        mean = np.dot(action, self.theta)\n",
    "        if self.model == 'gaussian':\n",
    "            return np.random.normal(mean, scale=self.var)\n",
    "        else:#add bernoulli model option\n",
    "            raise NotImplementedError('only Gaussian rewards are implemented so far')\n",
    "            \n",
    "    def get_means(self):\n",
    "        return np.dot(self.current_action_set, self.theta)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_PSPRfU-CRy"
   },
   "source": [
    "# Linear Bandit Agents\n",
    "\n",
    "For your report, **please export only the notebook from this cell on**. \n",
    "\n",
    "**Your Name**: XX XX\n",
    "\n",
    "**Your co-author's name**: XX XX / NA (leave NA if no co-author)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LinUCB : Implementing optimism in $R^d$\n",
    "\n",
    "As seen in class, the actions are now vectors in $R^d$, representing contextual features, and the environment is assumed to generate rewards according to some hidden linear function $f_\\theta(a) = a^\\top \\theta$. The goal of the learner is thus to estimate $\\theta$ while keeping a measure of the uncertainty in all the directions of the feature space. \n",
    "\n",
    "* **Baseline: Implementation of LinEpsilonGreedy** In the next cell, we implemented a LinUniform Agent that returns one of the action vectors of the action set, chosen uniformly at random. Please implement an LinEpsilonGreedy agent as seen in the previous Lab. Do you think these agents can have a sublinear regret ? \n",
    "\n",
    "*[Your answer]\n",
    "\n",
    "\n",
    "* **Implementation of LinUCB**: you need to compute UCBs for each arm of the current action set received from the environment, but this time the exploration bonus depends on the history of taken actions and received rewards. \n",
    "\n",
    "* **Efficiency of the matrix inversion step**: One key step is to invert the covariance matrix in order to compute the elliptical norm of each available action. Remark however that at round $t+1$, the new covariance matrix is very similar to the previous one at rount $t$... Can you think of a way to optimize this step by simply updating the old one ? \n",
    "Hint : You can search for a way to compute the inverse of the sum of an invertible matrix A and the outer product, $uv^\\top$, of vectors u and v.\n",
    "\n",
    "*[Your answer]\n",
    "\n",
    "\n",
    "* **Implement and additional exploration hyper-parameter**: It is common practice to modify LinUCB by multiplying the confidence bonus of each arm by some hyperparameter $ 0<\\alpha <1 $. \n",
    "Please implement this little modification of LinUCB. \n",
    "\n",
    "You may wonder why one would do that and it is a legitimate question. We guide you to explore this question further below, bear with us :)\n",
    "\n",
    "*[Your answer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9uEOaPwBAbA"
   },
   "outputs": [],
   "source": [
    "class LinUniform(Agent):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_action(self, arms):\n",
    "    K, _ = arms.shape\n",
    "    return arms[np.random.choice(K)]\n",
    "\n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    pass\n",
    "\n",
    "  def reset(self):\n",
    "    pass\n",
    "\n",
    "  #@staticmethod\n",
    "  def name(self):\n",
    "    return 'Unif'\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin-$\\epsilon$-Greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinEpsilonGreedy(Agent):\n",
    "  def __init__(self, d,lambda_reg, eps=0.1,):\n",
    "    self.eps = eps # exploration probability\n",
    "    self.d = d\n",
    "    self.lambda_reg = lambda_reg\n",
    "    self.reset()\n",
    "    \n",
    "  def reset(self):\n",
    "    self.t = 0\n",
    "    self.hat_theta = np.zeros(self.d)\n",
    "    self.cov = self.lambda_reg * np.identity(self.d)\n",
    "    self.invcov = np.identity(self.d)\n",
    "    self.b_t = np.zeros(self.d) \n",
    "\n",
    "  def get_action(self, arms):\n",
    "    K, _ = arms.shape\n",
    "    ## TODO: implement here\n",
    "    return arms[np.random.choice(K)]\n",
    "    \n",
    "\n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    \"\"\"\n",
    "    update the internal quantities required to estimate the parameter theta using least squares\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement updates here\n",
    "    self.t += 1   \n",
    "        \n",
    "\n",
    "  #@staticmethod\n",
    "  def name(self):\n",
    "    return 'LinEGreedy('+str(self.eps)+')'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin-UCB: The optimistic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinUCB(Agent):\n",
    "\n",
    "    def __init__(self, d, delta, lambda_reg, alpha=1.):\n",
    "        self.d = d\n",
    "        self.delta = delta\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.cov = self.lambda_reg * np.identity(d)\n",
    "        #TODO: instantiate alpha\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # reset all local variables that should not be kept when the experiment is restarted\n",
    "        self.t = 0\n",
    "        self.hat_theta = np.zeros(self.d)\n",
    "        self.cov = self.lambda_reg * np.identity(self.d)\n",
    "        self.invcov = np.identity(self.d)\n",
    "        self.b_t = np.zeros(self.d)  \n",
    "\n",
    "\n",
    "    def get_action(self, arms):\n",
    "        \"\"\"\n",
    "            This function implements LinUCB\n",
    "            Input:\n",
    "            -------\n",
    "            arms : list of arms (d-dimensional vectors)\n",
    "\n",
    "            Output:\n",
    "            -------\n",
    "            chosen_arm : vector (chosen arm array of features)\n",
    "            \"\"\"\n",
    "        # compute the UCB of each of the arm in arms, here arms are vectors\n",
    "        ucbs = np.array(K)\n",
    "\n",
    "        for i in range(K):\n",
    "            ucbs[i] = np.random.random()\n",
    "            # use hat_theta and beta and the covariance matrix. \n",
    "        \n",
    "        chosen_arm_index = np.argmax(self.UCBs)\n",
    "        chosen_arm = arms[chosen_arm_index]\n",
    "        return chosen_arm\n",
    "\n",
    "\n",
    "#     return np.random.choice(K)\n",
    "    \n",
    "\n",
    "    def receive_reward(self, chosen_arm, reward):\n",
    "        \"\"\"\n",
    "        update the internal quantities required to estimate the parameter theta using least squares\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def name(self):\n",
    "        return \"LinUCB(\"+str(self.alpha)+')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinTS : Taking the Bayesian way\n",
    "\n",
    "Thompson Sampling is a popular bayesian alternative to the standard optimistic bandit algorithms (see Chapter 36 of Bandit Algorithms). The key idea is to rely on Bayesian *samples* to get a proxy for the hidden parameter $\\theta$ of the problem instead of building high-probability confidence regions. \n",
    "\n",
    "* **Posterior derivation**: Let us place a Gaussian prior with mean $\\mathbf{0}$ and covariance $\\lambda I$ on $\\theta$. Given actions $A_1,\\ldots,A_t$ and rewards $Y_1,\\ldots,Y_t$, Can you compute the expression of the posterior at the beginning of round $t+1$ ? \n",
    "\n",
    "*[Your answer]\n",
    "\n",
    "\n",
    "* **Implementation of a LinTS (Linear Thompson Sampling) agent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinTS(Agent):\n",
    "\n",
    "  def __init__(self, d, delta, lambda_prior):\n",
    "    self.d = d\n",
    "    self.delta = delta\n",
    "    self.lambda_prior = lambda_prior\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    # reset all local variables that should not be kept when the experiment is restarted\n",
    "    self.t = 0\n",
    "    self.hat_theta = np.zeros(self.d)\n",
    "    self.cov = self.lambda_prior * np.identity(self.d)\n",
    "    self.invcov = np.identity(self.d)\n",
    "    self.b_t = np.zeros(self.d)  \n",
    "    \n",
    "\n",
    "  def get_action(self, arms):\n",
    "    \"\"\"\n",
    "        This function implements LinUCB\n",
    "        Input:\n",
    "        -------\n",
    "        arms : list of arms (d-dimensional vectors)\n",
    "\n",
    "        Output:\n",
    "        -------\n",
    "        chosen_arm : vector (chosen arm array of features)\n",
    "        \"\"\"    \n",
    "    \n",
    "    ## TODO\n",
    "    K, _ = arms.shape\n",
    "    return arms[np.random.choice(K)]\n",
    "\n",
    "\n",
    "    return chosen_arm\n",
    "\n",
    "    \n",
    "\n",
    "  def receive_reward(self, chosen_arm, reward):\n",
    "    \"\"\"\n",
    "    update the internal quantities required to estimate the parameter theta using least squares\n",
    "    \"\"\"\n",
    "    self.t += 1\n",
    "\n",
    "\n",
    "  def name(self):\n",
    "    return \"LinTS\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play !\n",
    "The function play runs one path of regret for one agent. The function experiment runs all agents several (Nmc) times and returns all the logged data. Feel free to check the inputs and outputs required when you decide on the implementation of your own agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChN6oiILAk2S"
   },
   "outputs": [],
   "source": [
    "def play(environment, agent, Nmc, T):\n",
    "    \"\"\"\n",
    "    Play one Nmc trajectories over a horizon T for the specified agent. \n",
    "    Return the agent's name (string) and the collected data in an nd-array.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.zeros((Nmc, T))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for n in range(Nmc):\n",
    "        agent.reset()\n",
    "        for t in range(T):\n",
    "            action_set = environment.get_action_set()\n",
    "            action = agent.get_action(action_set) \n",
    "            # Note that the main difference with the previous lab is that now get_action needs to receive the action_set\n",
    "            reward = environment.get_reward(action)\n",
    "            agent.receive_reward(action,reward)\n",
    "            \n",
    "            # compute instant regret\n",
    "            means = environment.get_means()\n",
    "            best_reward = np.max(means)\n",
    "            data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
    "            \n",
    "    return agent.name(), data\n",
    "\n",
    "\n",
    "def experiment(environment, agents, Nmc, T):\n",
    "    \"\"\"\n",
    "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent_id, regrets = play(environment, agent,Nmc, T)\n",
    "        \n",
    "        all_data[agent_id] = regrets\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default settings\n",
    "These should be good but feel free to try others, please highlight your changes wherever necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2  # dimension\n",
    "K = 5  # number of arms\n",
    "\n",
    "# parametor vector \\theta, normalized :\n",
    "# theta = randomActionsGenerator(1,d)\n",
    "theta = np.array([0.45, 0.5])\n",
    "theta /= np.linalg.norm(theta)\n",
    "\n",
    "T = 1000  # Finite Horizon\n",
    "N = 50  # Monte Carlo simulations\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "# save subsampled points for Figures\n",
    "Nsub = 100\n",
    "tsav = range(2, T, Nsub)\n",
    "\n",
    "#choice of quantile display\n",
    "q = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "0BoYONrQBAan",
    "outputId": "33d7f115-c725-4544-f2ea-47e68bf8ad30"
   },
   "outputs": [],
   "source": [
    "# three environments\n",
    "\n",
    "iid_env = LinearBandit(theta, K, pb_type='iid') \n",
    "\n",
    "fixed_actions = np.array(([1,0.1],[0.1,1],[0.3,0.4]))# or... #randomActionsGenerator(K,d)\n",
    "fixed_env = LinearBandit(theta, K, pb_type='fixed', fixed_actions=fixed_actions) # check if gaps are small\n",
    "print(fixed_env.get_means())\n",
    "\n",
    "arbitrary_env = LinearBandit(theta, K, pb_type='arbitrary')\n",
    "\n",
    "\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policies\n",
    "\n",
    "linucb = LinUCB(d, delta, lambda_reg=1.)\n",
    "uniform = LinUniform()\n",
    "e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1)\n",
    "greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AY1ZUny5I4Pl"
   },
   "source": [
    "# Example question: Which baseline is the strongest, LinEpsilonGreedy or LinUniform?\n",
    "This is a running demo, please follow this \"template\" in the questions below: 1/ experiment setup (choose environment(s), choose policies and parameters, 2/ plot, 3/ conclude by making observations on your results and responding to the question. If something catches your attention and you want to investigate further, please concisely write up what you want to test and why, as well as addtional conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iid environment\n",
    "e_greedy_vs_unif_iid = experiment(iid_env, [uniform, e_greedy], Nmc=N, T=T)\n",
    "\n",
    "plot_regret(e_greedy_vs_unif_iid, q=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## arbitrary environment\n",
    "e_greedy_vs_unif_arb = experiment(arbitrary_env, [uniform, e_greedy], Nmc=N, T=T)\n",
    "\n",
    "plot_regret(e_greedy_vs_unif_arb, q=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fixed-actions environment\n",
    "e_greedy_vs_unif_fixed = experiment(fixed_env, [uniform, e_greedy], Nmc=N, T=T)\n",
    "\n",
    "plot_regret(e_greedy_vs_unif_fixed, q=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: We tested LinEspilonGreedy and Uniform on all three environment types and LinEpsilonGreedy is stronger (it has lower regret) in all cases. However, we remark that its regret seems linear in all cases as expected due to the non-diminishing amount of exploration, **perhaps it would be interesting to see what happens when we remove this forced exploration?** We note that the difference between Uniform and EGreedy is significantly smaller in the fixed action problem, which is likely due to the reasonably small gaps (~0.05) in this problem (see env setup in above cell). \n",
    "\n",
    "**We quickly run the experiment suggested above, to check how Greedy ($\\epsilon=0$) performs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_iid = experiment(iid_env, [greedy, e_greedy], Nmc=N, T=T)\n",
    "greedy_arb = experiment(arbitrary_env, [greedy, e_greedy], Nmc=N, T=T)\n",
    "greedy_fixed = experiment(fixed_env, [greedy, e_greedy], Nmc=N, T=T)\n",
    "\n",
    "plot_regret(greedy_iid, q=10)\n",
    "plot_regret(greedy_arb, q=10)\n",
    "plot_regret(greedy_fixed, q=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the arbitrary and iid environments, Greedy seems to be learning! This is surprising and could be investigated further but it is beyond the present question's scope. In the fixed-actions environment, however, greedy has a linear regret. **We conclude that Greedy may actually be quite a strong baseline in iid and arbitrary environments**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Is LinUCB better than the baseline LinEpsilonGreedy? \n",
    "Compare the two methods on all environments and conclude. In light of the above example, decide which values of epsilon you want to try in your comparisons and explain your choices following the suggested template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is the role of $\\alpha$ for LinUCB ?\n",
    "\n",
    "In the question above, we saw that the fixed-actions setting is the most interesting because it is the most difficult. For this question, we fix the environment to fixed-actions. \n",
    "Compare the behavior of various instantiations of LinUCB using different values of the scaling hyper-parameter $\\alpha$ and conclude on its usefulness / risks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Is Thompson Sampling better than LinUCB?\n",
    "Compare these two policies in all environments and conclude. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StochasticLinearBandits.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
